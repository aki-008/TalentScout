{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eee01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08942e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello. How can I help you today?' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 36, 'total_tokens': 46, 'completion_time': 0.028439477, 'prompt_time': 0.001720475, 'queue_time': 0.067997378, 'total_time': 0.030159952}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_9a8b91ba77', 'finish_reason': 'stop', 'logprobs': None} id='run--ad1faee5-ec1a-4c96-96c8-5506d6e193e8-0' usage_metadata={'input_tokens': 36, 'output_tokens': 10, 'total_tokens': 46}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.7,\n",
    "    # max_tokens=1000\n",
    ")\n",
    "\n",
    "res = llm.invoke(\"hello\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b19c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from typing import Literal, TypedDict, List, Optional, Dict\n",
    "from typing_extensions import Annotated\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "import os\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import logging\n",
    "\n",
    "# from prompts import (Questionnaire_prompt, report_planner_query_writer)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[logging.FileHandler(\"TS.log\"), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86f8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START hello\n",
      "\n",
      "Hello! My name is Devin, a Talent Scout Assistant here at SkillZilla. It's great to speak with you today. Thanks for taking the time to apply for the [Job Title] position. I'm assisting Aria, our HR Manager, with the candidate screening process. \n",
      "\n",
      "Could you please tell me a little about yourself, your background, and what interests you about this particular role at SkillZilla?  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a65ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Based \"\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm\n",
    "\n",
    "res = chain.invoke({\"input\": \"hello\"})\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ddd398",
   "metadata": {},
   "source": [
    "## Resume parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2516945a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shaswat Singh \n",
      "AI DEVELOPER \n",
      "Lucknow, UP, India | 9140529926 | Shaswatsingh910@gmail.com | LinkedIn | GitHub| Medium | Website \n",
      " \n",
      "Objective \n",
      "An aspiring AI developer and software engineer with a passion for building intelligent systems \n",
      "that automate and enhance real-world processes. Experienced in developing AI-powered \n",
      "solutions, fine-tuning models, and deploying applications on cloud infrastructure. Skilled in \n",
      "integrating natural language processing (NLP), computer vision, and large language models into \n",
      "innovative projects. \n",
      " \n",
      "Education \n",
      "CGPA: 6.9 \n",
      " \n",
      "University of Lucknow | Bachelor of Computer Application                       2022-2025 \n",
      " \n",
      "Relevant Coursework \n",
      "Data Structures, Software Engineering, Algorithms, Database Management, Computer Networks, \n",
      "Operating System, Artificial Intelligence, Machine Learning \n",
      "Skills & abilities \n",
      " \n",
      "â€¢ \n",
      "Programming Languages: Python \n",
      "â€¢ \n",
      "AI/ML Frameworks: Hugging Face, Langhian, Llama Index, Unsloth, Transformers, FastAPI, \n",
      "Numpy, pandas, seaborn, PyTorch, vllm, LM-Deploy \n",
      "â€¢ \n",
      "Development Tools: Git, Ollama, Ngrok, Jupyter Notebook, Vercel \n",
      "â€¢ \n",
      "Database: ChromaDB, MySQL, Qdrant \n",
      "â€¢ \n",
      "Currently Learning: Flask, Web-Dev \n",
      " \n",
      "Projects \n",
      "DevEcho | AI-Powered Social Media Assistant \n",
      "An intelligent assistant that automates LinkedIn post generation from user-provided content using LLMs \n",
      "and Retrieval-Augmented Generation (RAG). Integrated data collection pipelines for topical research. \n",
      "Tech Stack: Python, LangChain, Groq, ChromaDB, Telegram API \n",
      "GUARDIAN-EYE.AI | Real-Time Violence Detection System \n",
      "Designed an AI and computer vision-based system for public safety in urban India. Detects violent \n",
      "activities in real-time and alerts authorities via Telegram, enhancing traditional surveillance systems. \n",
      "Tech Stack: Python, YOLOv8, Telegram API, OpenCV, Custom LLMs \n",
      "CSVision | AI-Based CSV Analysis Tool \n",
      "Built a web-based platform enabling users to query CSV files using natural language. Leverages Groqâ€™s \n",
      "LLMs to return insights and generate visualizations. \n",
      "Tech Stack: Huggingface Spaces, Groq API, Streamlit \n",
      "Wanderlust | AI Travel Itinerary Generator \n",
      "Created an interactive AI assistant that builds personalized travel itineraries based on user preferences \n",
      "via a dynamic Q&A flow. \n",
      "Tech Stack: Python, LangChain, Prompt Engineering \n",
      "Git_RAG | GitHub Repo RAG Assistant \n",
      "Implemented a pipeline to ingest and process GitHub repositories, enabling users to ask questions about \n",
      "the code using RAG and LLMs. \n",
      "Tech Stack: Python, LangChain, Llamaindex, GitIngest, ChromaDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"USER_AGENT\"] = \"TalentScout\" \n",
    "\n",
    "def resume_loader(path_to_resume: str):\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "                path_to_resume,\n",
    "                glob=\"**/*.pdf\",\n",
    "                loader_cls=PyMuPDFLoader,\n",
    "                show_progress=True,  # Optional: Displays a progress bar\n",
    "                use_multithreading=True  # Optional: Enables multithreading for faster loading\n",
    "            )\n",
    "\n",
    "    langchian_loader = loader.load() \n",
    "    resume_data =langchian_loader[0].page_content\n",
    "    return resume_data\n",
    "\n",
    "\n",
    "\n",
    "resume_data = resume_loader(\"user_data\")\n",
    "print(resume_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a612c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(resume_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a95348b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shaswat Singh \n",
      "AI DEVELOPER \n",
      "Lucknow, UP, India | 9140529926 | Shaswatsingh910@gmail.com | LinkedIn | GitHub| Medium | Website \n",
      " \n",
      "Objective \n",
      "An aspiring AI developer and software engineer with a passion for building intelligent systems \n",
      "that automate and enhance real-world processes. Experienced in developing AI-powered \n",
      "solutions, fine-tuning models, and deploying applications on cloud infrastructure. Skilled in \n",
      "integrating natural language processing (NLP), computer vision, and large language models into \n",
      "innovative projects. \n",
      " \n",
      "Education \n",
      "CGPA: 6.9 \n",
      " \n",
      "University of Lucknow | Bachelor of Computer Application                       2022-2025 \n",
      " \n",
      "Relevant Coursework \n",
      "Data Structures, Software Engineering, Algorithms, Database Management, Computer Networks, \n",
      "Operating System, Artificial Intelligence, Machine Learning \n",
      "Skills & abilities \n",
      " \n",
      "â€¢ \n",
      "Programming Languages: Python \n",
      "â€¢ \n",
      "AI/ML Frameworks: Hugging Face, Langhian, Llama Index, Unsloth, Transformers, FastAPI, \n",
      "Numpy, pandas, seaborn, PyTorch, vllm, LM-Deploy \n",
      "â€¢ \n",
      "Development Tools: Git, Ollama, Ngrok, Jupyter Notebook, Vercel \n",
      "â€¢ \n",
      "Database: ChromaDB, MySQL, Qdrant \n",
      "â€¢ \n",
      "Currently Learning: Flask, Web-Dev \n",
      " \n",
      "Projects \n",
      "DevEcho | AI-Powered Social Media Assistant \n",
      "An intelligent assistant that automates LinkedIn post generation from user-provided content using LLMs \n",
      "and Retrieval-Augmented Generation (RAG). Integrated data collection pipelines for topical research. \n",
      "Tech Stack: Python, LangChain, Groq, ChromaDB, Telegram API \n",
      "GUARDIAN-EYE.AI | Real-Time Violence Detection System \n",
      "Designed an AI and computer vision-based system for public safety in urban India. Detects violent \n",
      "activities in real-time and alerts authorities via Telegram, enhancing traditional surveillance systems. \n",
      "Tech Stack: Python, YOLOv8, Telegram API, OpenCV, Custom LLMs \n",
      "CSVision | AI-Based CSV Analysis Tool \n",
      "Built a web-based platform enabling users to query CSV files using natural language. Leverages Groqâ€™s \n",
      "LLMs to return insights and generate visualizations. \n",
      "Tech Stack: Huggingface Spaces, Groq API, Streamlit \n",
      "Wanderlust | AI Travel Itinerary Generator \n",
      "Created an interactive AI assistant that builds personalized travel itineraries based on user preferences \n",
      "via a dynamic Q&A flow. \n",
      "Tech Stack: Python, LangChain, Prompt Engineering \n",
      "Git_RAG | GitHub Repo RAG Assistant \n",
      "Implemented a pipeline to ingest and process GitHub repositories, enabling users to ask questions about \n",
      "the code using RAG and LLMs. \n",
      "Tech Stack: Python, LangChain, Llamaindex, GitIngest, ChromaDB\n"
     ]
    }
   ],
   "source": [
    "print(resume_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0476b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import XMLOutputParser, JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d92227d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 09:38:54,107 - __main__ - INFO - Node: Make Question started\n",
      "2025-07-16 09:38:56,009 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_name': 'Shaswat Singh', 'email': 'Shaswatsingh910@gmail.com', 'phone': '9140529926', 'linkedin': '', 'github': '', 'portfolio_website': 'Website', 'location': 'Lucknow, UP, India', 'education': [{'degree': 'Bachelor of Computer Application', 'field_of_study': '', 'university': 'University of Lucknow', 'start_year': '2022', 'end_year': '2025'}], 'work_experience': [], 'skills': ['Python', 'Hugging Face', 'Langhian', 'Llama Index', 'Unsloth', 'Transformers', 'FastAPI', 'Numpy', 'pandas', 'seaborn', 'PyTorch', 'vllm', 'LM-Deploy', 'Git', 'Ollama', 'Ngrok', 'Jupyter Notebook', 'Vercel', 'ChromaDB', 'MySQL', 'Qdrant', 'Flask', 'Web-Dev'], 'certifications': [], 'projects': [{'name': 'DevEcho | AI-Powered Social Media Assistant', 'description': 'An intelligent assistant that automates LinkedIn post generation from user-provided content using LLMs and Retrieval-Augmented Generation (RAG). Integrated data collection pipelines for topical research.', 'technologies': ['Python', 'LangChain', 'Groq', 'ChromaDB', 'Telegram API']}, {'name': 'GUARDIAN-EYE.AI | Real-Time Violence Detection System', 'description': 'Designed an AI and computer vision-based system for public safety in urban India. Detects violent activities in real-time and alerts authorities via Telegram, enhancing traditional surveillance systems.', 'technologies': ['Python', 'YOLOv8', 'Telegram API', 'OpenCV', 'Custom LLMs']}, {'name': 'CSVision | AI-Based CSV Analysis Tool', 'description': 'Built a web-based platform enabling users to query CSV files using natural language. Leverages Groqâ€™s LLMs to return insights and generate visualizations.', 'technologies': ['Huggingface Spaces', 'Groq API', 'Streamlit']}, {'name': 'Wanderlust | AI Travel Itinerary Generator', 'description': 'Created an interactive AI assistant that builds personalized travel itineraries based on user preferences via a dynamic Q&A flow.', 'technologies': ['Python', 'LangChain', 'Prompt Engineering']}, {'name': 'Git_RAG | GitHub Repo RAG Assistant', 'description': 'Implemented a pipeline to ingest and process GitHub repositories, enabling users to ask questions about the code using RAG and LLMs.', 'technologies': ['Python', 'LangChain', 'Llamaindex', 'GitIngest', 'ChromaDB']}], 'languages': []}\n"
     ]
    }
   ],
   "source": [
    "from prompts import resume_parser_prompt\n",
    "import re\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def clean_XML(text):\n",
    "    text = re.sub(r'[^\\x09\\x0A\\x0D\\x20-\\x7F]', '', text)\n",
    "    return text\n",
    "\n",
    "def Parse_resume(resume_data:str):\n",
    "\n",
    "    logger.info('Node: Make Question started')\n",
    "    data = resume_data\n",
    "    parser = JsonOutputParser()\n",
    "    parser.get_format_instructions()\n",
    "    prompt = PromptTemplate(\n",
    "        template=(resume_parser_prompt),\n",
    "        input_variables=[\"RESUME\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm |parser\n",
    "    parsed_resume = chain.invoke({\"RESUME\": data})\n",
    "    return parsed_resume\n",
    "res = Parse_resume(resume_data)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f30da4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_name': 'Shaswat Singh', 'email': 'Shaswatsingh910@gmail.com', 'phone': '9140529926', 'linkedin': '', 'github': '', 'portfolio_website': 'Website', 'location': 'Lucknow, UP, India', 'education': [{'degree': 'Bachelor of Computer Application', 'field_of_study': '', 'university': 'University of Lucknow', 'start_year': '2022', 'end_year': '2025'}], 'work_experience': [], 'skills': ['Python', 'Hugging Face', 'Langhian', 'Llama Index', 'Unsloth', 'Transformers', 'FastAPI', 'Numpy', 'pandas', 'seaborn', 'PyTorch', 'vllm', 'LM-Deploy', 'Git', 'Ollama', 'Ngrok', 'Jupyter Notebook', 'Vercel', 'ChromaDB', 'MySQL', 'Qdrant', 'Flask', 'Web-Dev'], 'certifications': [], 'projects': [{'name': 'DevEcho | AI-Powered Social Media Assistant', 'description': 'An intelligent assistant that automates LinkedIn post generation from user-provided content using LLMs and Retrieval-Augmented Generation (RAG). Integrated data collection pipelines for topical research.', 'technologies': ['Python', 'LangChain', 'Groq', 'ChromaDB', 'Telegram API']}, {'name': 'GUARDIAN-EYE.AI | Real-Time Violence Detection System', 'description': 'Designed an AI and computer vision-based system for public safety in urban India. Detects violent activities in real-time and alerts authorities via Telegram, enhancing traditional surveillance systems.', 'technologies': ['Python', 'YOLOv8', 'Telegram API', 'OpenCV', 'Custom LLMs']}, {'name': 'CSVision | AI-Based CSV Analysis Tool', 'description': 'Built a web-based platform enabling users to query CSV files using natural language. Leverages Groqâ€™s LLMs to return insights and generate visualizations.', 'technologies': ['Huggingface Spaces', 'Groq API', 'Streamlit']}, {'name': 'Wanderlust | AI Travel Itinerary Generator', 'description': 'Created an interactive AI assistant that builds personalized travel itineraries based on user preferences via a dynamic Q&A flow.', 'technologies': ['Python', 'LangChain', 'Prompt Engineering']}, {'name': 'Git_RAG | GitHub Repo RAG Assistant', 'description': 'Implemented a pipeline to ingest and process GitHub repositories, enabling users to ask questions about the code using RAG and LLMs.', 'technologies': ['Python', 'LangChain', 'Llamaindex', 'GitIngest', 'ChromaDB']}], 'languages': []}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f65e2a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open (\"resume.json\", \"w\") as f:\n",
    "    json.dump(res, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e572f",
   "metadata": {},
   "source": [
    "## AGENT greet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4613bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 11:22:10,479 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm janus, an AI assistant working under radhika. My role is to help evaluate and shortlist candidates in a structured, unbiased, and data-driven way.  \n",
      "\n",
      "Please upload a resume or provide candidate details so I can begin the evaluation process.  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prompts import greet_prompt_template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "        template=(greet_prompt_template),\n",
    "        input_variables=[\"candidate_name\", \"agent_name\", \"hr_manager_name\"],\n",
    "        # partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "res = chain.invoke({\"input\":\"START\",\"candidate_name\": \"John Doe\", \"agent_name\":\"janus\", \"hr_manager_name\": \"radhika\"})\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a42e1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 11:11:49,586 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understood! I am ready to assist you in evaluating candidates.  \n",
      "\n",
      "Please provide me with the resume or candidate details. I will follow your instructions carefully and provide objective evaluations based on the information given.  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prompts import greet_prompt_template\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "        template=(greet_prompt_template),\n",
    "        input_variables=[\"input\",\"candidate_name\", \"agent_name\", \"hr_manager_name\"]\n",
    "        # partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "res = chain.invoke({\" input\":\"START\",\"candidate_name\":\"Shaswat\",\"agent_name\":\"HireBot\",\"hr_manager_name\":\"Ms. Priya Sharma\"})\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570cbf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_question():\n",
    "    \"\"\"Generate exactly 3 questions that help understand the user's research intent from the query.\n",
    "    The questions should be easy to answer in one word or a short sentence, free of jargon and complexity,\n",
    "    and focused on enriching the research plan (goal, focus area, scope, time).\"\"\"\n",
    "\n",
    "    logger.info('Node: Make Question started')\n",
    "    query = state[\"initial_query\"]\n",
    "    questions = {}\n",
    "    num = 3\n",
    "    parser = JsonOutputParser()\n",
    "    prompt = PromptTemplate(\n",
    "        template=(Questionnaire_prompt),\n",
    "        input_variables=[\"num\", \"query\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    try:\n",
    "        logger.info(f'Node: Making questions on {query}')\n",
    "        questions = chain.invoke({\"num\": num, \"query\": query})\n",
    "        logger.info(f'Generated questions: {questions}')    \n",
    "        if not isinstance(questions, dict):\n",
    "            logger.info(f'Node: LLM generated questions are not a dictionary: Output: {questions}')\n",
    "            return{\"query_questions\": {}}    \n",
    "        return {\"query_questions\": questions}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate questions: {e}\")\n",
    "        return {\"query_questions\": {}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4333c5cb",
   "metadata": {},
   "source": [
    "## Generate tech questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1bcf9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from prompts import tech_questions\n",
    "\n",
    "\n",
    "\n",
    "def Tech_question(resume_data):\n",
    "    \"\"\"Generates clarifying questions based on the initial query using an LLM.\"\"\"\n",
    "    # logger.info(\"Node: make_question execution started.\")\n",
    "    questions = {}\n",
    "    num = 3 # Number of questions to generate\n",
    "    parser = JsonOutputParser()\n",
    "    # Corrected Example Output to be valid JSON\n",
    "    prompt = PromptTemplate(\n",
    "        template=(tech_questions),\n",
    "        input_variables=[\"num\", \"resume_data\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "\n",
    "    questions = chain.invoke({\"num\": num, \"tech_stack\": resume_data})\n",
    "    return questions\n",
    "\n",
    "result = Tech_question(resume_data)\n",
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e3226ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q1': 'Describe the role of LangChain in building an AI-powered application.', 'q2': 'How do you utilize ChromaDB for efficient knowledge retrieval in your AI projects?', 'q3': 'Explain the difference between using PyTorch and Transformers for building a machine learning model.'}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8922251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd1af8",
   "metadata": {},
   "source": [
    "## Get answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b4e1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers(questions):\n",
    "    answers = {}\n",
    "    for i in questions:\n",
    "        answers[questions[i]] = input(f\"Question {i}: {questions[i]}\")\n",
    "    print(answers)\n",
    "    print(type(answers))\n",
    "    return answers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e2fde56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Describe the role of LangChain in building an AI-powered application.': '45', 'How do you utilize ChromaDB for efficient knowledge retrieval in your AI projects?': '45', 'Explain the difference between using PyTorch and Transformers for building a machine learning model.': '4'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "start = {}\n",
    "start = answers(ques)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b448601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_answers = {\n",
    "    \"Describe the role of LangChain in building an AI-powered application.\": \n",
    "        \"LangChain acts as an orchestration framework for building AI-powered applications by enabling developers to connect language models with external data sources and tools. It provides abstractions for prompt templates, chains, agents, memory, and tools, making it easier to manage complex workflows involving LLMs. In essence, it simplifies integrating and controlling how LLMs interact with documents, APIs, databases, or user input in a structured way.\",\n",
    "\n",
    "    \"How do you utilize ChromaDB for efficient knowledge retrieval in your AI projects?\": \n",
    "        \"ChromaDB is used as a vector database for storing and retrieving embeddings of documents or knowledge chunks. In AI projects, it enables efficient semantic search by allowing fast similarity searches using vector representations of queries. This makes it ideal for Retrieval-Augmented Generation (RAG) pipelines, where relevant documents are retrieved from ChromaDB based on the userâ€™s query and then passed to the language model for context-aware response generation.\",\n",
    "\n",
    "    \"Explain the difference between using PyTorch and Transformers for building a machine learning model.\": \n",
    "        \"PyTorch is a deep learning framework used to build and train custom neural network models from scratch. It provides low-level control over model architecture, training loops, and optimization. On the other hand, the Transformers library (by Hugging Face) is a high-level library built on top of PyTorch (and optionally TensorFlow) that provides pre-trained transformer-based models like BERT, GPT, etc., with simple APIs for fine-tuning and inference. In summary, PyTorch is foundational and flexible, while Transformers is specialized and optimized for NLP tasks using state-of-the-art models.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16927ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Describe the role of LangChain in building an AI-powered application.': 'LangChain acts as an orchestration framework for building AI-powered applications by enabling developers to connect language models with external data sources and tools. It provides abstractions for prompt templates, chains, agents, memory, and tools, making it easier to manage complex workflows involving LLMs. In essence, it simplifies integrating and controlling how LLMs interact with documents, APIs, databases, or user input in a structured way.', 'How do you utilize ChromaDB for efficient knowledge retrieval in your AI projects?': 'ChromaDB is used as a vector database for storing and retrieving embeddings of documents or knowledge chunks. In AI projects, it enables efficient semantic search by allowing fast similarity searches using vector representations of queries. This makes it ideal for Retrieval-Augmented Generation (RAG) pipelines, where relevant documents are retrieved from ChromaDB based on the userâ€™s query and then passed to the language model for context-aware response generation.', 'Explain the difference between using PyTorch and Transformers for building a machine learning model.': 'PyTorch is a deep learning framework used to build and train custom neural network models from scratch. It provides low-level control over model architecture, training loops, and optimization. On the other hand, the Transformers library (by Hugging Face) is a high-level library built on top of PyTorch (and optionally TensorFlow) that provides pre-trained transformer-based models like BERT, GPT, etc., with simple APIs for fine-tuning and inference. In summary, PyTorch is foundational and flexible, while Transformers is specialized and optimized for NLP tasks using state-of-the-art models.'}\n"
     ]
    }
   ],
   "source": [
    "print(sim_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f0ab5f",
   "metadata": {},
   "source": [
    "## Get results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e0dd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 13:50:11,053 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from prompts import Evaluator_prompt\n",
    "def Get_reults(answers: dict):\n",
    "    \"\"\"Generates clarifying questions based on the initial query using an LLM.\"\"\"\n",
    " \n",
    "\n",
    "    # parser = JsonOutputParser()\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=(Evaluator_prompt),\n",
    "        input_variables=[\"qa\"],\n",
    "        # partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm \n",
    "\n",
    "\n",
    "    questions = chain.invoke({\"qa\": answers})\n",
    "    return questions\n",
    "\n",
    "result = Get_reults(sim_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec67d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58319d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: TC=5, AC=5, DC=5\n",
      "Q2: TC=5, AC=5, DC=5\n",
      "Q3: TC=5, AC=5, DC=5\n",
      "Total Score: 45 out of 45\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85244841",
   "metadata": {},
   "source": [
    "## Sendoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc44974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thank you for completing the initial steps of the hiring process, Jan. ðŸ™Œ\n",
      "    Our team will carefully evaluate your responses and resume.\n",
      "    You will be notified via email or message if you are selected for the next round.\n",
      "    We appreciate your patience and interest in this opportunity!\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from prompts import sendoff_node_prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def sendoff_node(candidate_name):\n",
    "\n",
    "    message = f\"\"\"\n",
    "    Thank you for completing the initial steps of the hiring process, {candidate_name}. ðŸ™Œ\n",
    "    Our team will carefully evaluate your responses and resume.\n",
    "    You will be notified via email or message if you are selected for the next round.\n",
    "    We appreciate your patience and interest in this opportunity!\n",
    "    \"\"\"\n",
    "\n",
    "    print(message)\n",
    "    \n",
    "    \n",
    "sendoff_node(candidate_name = \"Jan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56053ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
